---
sidebar_position: 3
---
# Build an Encoder-Decoder Triplet Extractor with HuggingFace

In this short guide, we're going to use the popular [Transformers](https://huggingface.co/docs/transformers/index) library by [HuggingFace](https://huggingface.co/) to build an autoregressive encoder-decoder triplet extraction model by fine-tuning [REBEL](https://github.com/Babelscape/rebel).

## Build Corpus

Here we'll convert the output of QuickGraphs download for a project with entity and relation annotaton into a suitable format for encoder-decoder language modelling.

## Model Fine-Tuning

## Model Inference

In this short guide we've seen that the output of QuickGraph can easily be converted into a format that is useful for achieving high performance for autoregressive triplet extraction.
